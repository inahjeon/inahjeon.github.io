---
layout: post
title:  "Devfest2019"
date:   2019-10-20 16:46:00
tags: [data science, embedding, devfest2019, kaggle]
comments: true
---

# 쉽게 따라할 수 있는 한국어 임베딩 구축 - 이기창

## 단어 임베딩으로 문서 분류하기

- 가장 많이 사용: fastText

### 핵심 컨셉
- 문서에 속한 단어가 유사하면 문서 의미도 비슷하다.

### 문서 벡터를 어떻게 만드나?
- 단어 임베딩의 합(sum)으로 문서 백터를 표현한다. (단어 벡터의 합 = 단어 벡터의 평균 = 문서 벡터의 중심)

### 실험으로 검증
- 네이버 영화 리뷰 말뭉치 활용
- 학습데이터(문장, label) 댓글을 모두 문서 벡터(단어 임베딩의 합)로 변환한다.
- cosine similarity 가 가장 높은 학습 데이터의 label로 분류

### 시사점
- 복잡한 딥러닝 모델을 써도 80%대 성능 기록
- 단어 임베딩 품질이 좋으면 자연어처리 성능을 높일 수 있음

## 임베딩이 어떻게 의미를 가지는가

- 말뭉치의 통계적 패턴이 들어있음

### Back-of-Words
- 단어 빈도를 센다.
- 문서를 쓴이의 의도는 단어 사용 패턴에 드러난다.

### ELMo, Bert
- 단어가 어떤 순서로 나타나는지 살핀다. (bi-direction)
- 시퀀스 정보에 ㅡ이미가 녹아있다.

### Word2Vec, FastText, GloVe
- 단어가 어떤 단어와 주로 같이 나타나는지 살핀다.
- 문맥에 의미가 녹아있다.

## 문장 수준 임베딩 구축 및 활용 (요즘 대세): ELMo, BERT

- 2014년은 워드 임베딩, 그 이후로 elmo (sentence embedding) 쪽으로 많이 연구되고 있음.

- ELMo: bidirectional LSTM, + CNN (다음 단어를 예측)

- BERT: (SOTA) Transformer network (Attention is all you need)

### 문장 수준 임베딩
- 문장 수준 임베딩의 장점은 동음이의어 분간 가능
- 문장의 문맥적 의미를 벡터화할 수 있음.

- 토큰화 > vocabulary > 학습

#### BERT
- https://github.com/yeontaek/BERT-Korean-Model
- SKT: https://github.com/SKTBrain/KoBERT

### 임베딩 활용
- 임베딩이 가장 크게 쓰일 수있는 분야는 전이학습. 
- 다른 네트워크의 입력값으로 사용돼 자연어 처리 성능을 높일 수 있음

- Bengio > Data Augmentation > Mixup

# Clean Code for ML/AI - 한성민

## What is Clean Code

깨진 유리창 이론 ~ 코드 품질

보이 스카웃 규칙 - 떠날 때는 찾을 때 보다 캠프장을 더욱 께끗이 할 것.

## Usecase

코드 악취 - 코드에 문제가 발생할 때 생기는 나쁜냄새

### 코드 악취 유형 1
- branch는 코드 복잡도를 높이므로 너무 많이 사용하지 않는 것 이 좋음.
- guard clause 패턴으로 코드를 아예 실행시키지 않도록 함.

### 코드 악취2 - 주석을 남용
- 자세하고 친절한 주석을 함수 이름으로 바꿔보세요.

### 코드 악취 3 - 중복코드
- 중복되는 코드 합치기

### 그밖의 코드 악취 케이스
- 너무 긴 메소드
- 너무 거대한 클래스
- 상속 거부: 자식 클래스에서 부모 클래스의 규칙을 무시하고 오버라이드
- 과도한 복잡성
- 게으른 클래스: 클래스에 부여된 기능이 너무 적은 경우

### 실제 모델 코드 최적화: Attention-based bidirectional LSTM
- 안쓰는 키워드 제거
- 직접 기입되어 있는 설정 인수들을 전역 변수로 추출 (수정이 빈번하게 일어남)
- 복잡성이 높은 로직은 함수로 추출
- 조건식 단순화, 중복 코드 함수 추출 (모델 코드의 경우 조건을 사용하는 경우가 많음)
- 모델 혹은 옵티마이저의 인수정보들을 전역 변수에서 설정 프레임워크를 이용한 파라미터로 전환 (google/gin-config)
- 코드 컨벤션 공통화
    - single, double quotation
    - 함수 사이 2줄
    - 1줄을 넘어서는 함수 인수의 intent

### configuration 관리툴
- https://github.com/IDSIA/sacred

### Lint, Quality gate
- python lint 도구: pylint, flake8, pycodestyle
- Quality gate 도구: code grade를 알려줌. (sonarqube, codeclimate, codebeat)

## The anti-patterns

- if/switch 절의 남용 -> key access를 통한 분기대체
- 데이터 초기화와 데이터 삽입 로직의 분리: 고차함수를 이용한 대체
> ex) 빈 배열 > append로 배열 채움.
- 타입(type)의 부재

### Q&A
- 모델 테스트: 기존 테스트의 결과와 유사도로 검증 (일정 유사도 이하면 error)

# BERT in kaggle - 이유한, 이영수, 송원호

## Transformer
- Encoder - Decoder 구조임 (seq-to-seq NMT에서 많이 사용)

### Encoder

- self-attension, feed forword network가 있음

### self-attension
- sealed dot product
-> (Q,K 내적)유사도를 구해서 scalaing

## BERT
- Pretraining (masked LM, Next sentence prediction)
- Fine-tuning

- Transformer의 Encoder만 사용함.

### image domain과 비교
- CNN -> self-attention
- Residual Block -> Encoder Block
- RESNET -> transformer
- RESNET(pretrained) -> BERT (pretrained)

## Toxic Jigsaw unintended bias in tocity classification
- 정상 코멘트(문장, 문단, 단어,..)인지 악성 코멘트인지 분류하는 문제
- 특정 키워드가 포함되면 악성으로 구분할 확률이 상당히 높아짐. (정상인데 악성으로 분류되는 경우)
- 대회에서 키워드들을 제시함.
- Metric: AUC, bias-AUC (특정 키워드가 포함되었지만 정상 코멘트를 잘 찾아내는 척도)

### 진행방법
- Baseline: Fasttext -> RNN
- evaluation EDA를 진행
- 커스텀 loss function을 정의함.
- 문맥에 따라 임베딩을 다르게 함. -> BERT를 활용
- Ensemble 사용

### Conclusion
- 좋은 머신은 꼭 필요하다.
- 작은 모델 뿐 아니라 큰 모델 역실 중요하다
- Evaluation에 맞는 loss를 잘 정의해야한다.
- 일찍 파이프라인을 구성하고 많은 실험을 시도한다.
- 제한된 시간 하에 높ㅇ느 점수를 내야해서 속도를 줄이기 위한 여러 기술이 필요하다.

## Molucle

### 분자 학습을 어떻게 시킬까?

#### 왜 다양한 딥러닝 아키텍쳐가 필요할까?
- 일반 머신러닝은 데이터를 표현하는데 한계가 있음 (LeCun)
- 주어진 데이터 내에서 피쳐를 잘 추출해내기 위함
- 데이터가 가진 정보는 다양한 형태, 관계, 성질을 가지기 때문

#### 분자 -> 그래프로 표현
- 문장으로 표현해본다면?
- 분자가 가진 여러 정보들을 문장으로 표현 (e.g. 탄소 4개가 있음 등)
- 각각의 정보들을 임베딩해 vector로 만듦.
- vector를 이어붙여 float sentence 생성

### 캐글에서 높은 점수를 얻으려면?

#### (1) with diversity, do ensemble!
- difreret model -> different layer, different hidden dim
- different seed -> different training subset

#### (2) Pseudo labeling
- 우리가 예측한 값을 target으로 한 test set으로 새로 학습하는것

#### (3) 함께 하라!

### 모델의 성능을 높이기 위해 생각해야할 것!
- 데이터의 핵심 피쳐를 잘 표한할 수 있는 학습 방식을 선택하라. (CNN, RNN, GNN, Transformer, Embedding)
- 데이터를 잘 학습할 수 있는 Loss function을 찾아라 (다 해봐야할).
- 학습이 잘 될 수 있는 조건을 최적화시켜라 (학습 스케쥴, 구조 최적화 등)
